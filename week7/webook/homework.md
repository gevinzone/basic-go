# 实现思路

## 基础方案

思路：把全部点赞数据，放到redis的zset中，score为(点赞数)*(-1)，点赞数前N的文章，即为通过zrange 查到的第0~N-1个元素

### 实现逻辑

1. 服务实例启动时，加载全部文章的点赞数到redis的zset中，注意：
   1. 这里要用分布式锁和double check确保只加载一次数据到redis
   2. 如果某个实例启动时，发现zset已经存在了，跳过加载操作，使用已有数据
   3. 数据库中数据要分片查询写入，查询语句类似 `select id from interactive where biz='article_like' and id > 200 limit 100`，其中，sql中的`100`代指每个批次的大小，`200`代指上个批次最后一条记录到id，这里要求表`interactive`的id是自增的
   4. 处于简化，该zset无过期时间，一直留在内存里
2. 查询点赞数前N的数据时，通过zrange返回第0~N-1个member
3. 文章有新的点赞或取消点赞时，通过ZINCRBY给zset中对应文章执行一致操作，注意这里数据一致性取舍如下：
   1. 缓存中的点赞数无需与数据库保持严格一致
   2. 先写数据库后写缓存即可，若写缓存失败，重试几次，重试也失败则
4. 为方式缓存服务器挂掉恢复后数据丢失等问题，提供接口支持手动加载该zset
5. 为防止缓存数据与数据库数据偏差较大，可以考虑做如下优化：
   1. 方案1，半夜定时删除缓存数据，并重新从数据库捞数据到缓存，方式与上面一样
   2. 方案2，懒加载+缓存过期
   3. 详情见“实现优化”

### 实现优化

方案1:

为确保半夜只对缓存更新过一次，有两个方案可选：
1. 执行更新操作时，向数据库中写如一条更新记录，关键字段为日期+版本号（如20231102，1），配置唯一联合索引，能写入成功的更新缓存
2. 缓存从无过期时间改为有24小时过期时间，半夜更新缓存时，如果缓存失效，或缓存失效时间与当前时间之差小于阈值，可以更新成功，这里要用分布式锁，防止并发问题

方案2：
1. 设计为懒加载方式，即需要访问该zset时，由于该zset key不存在，需要从数据库中捞全部数据，这里也要用分布式锁和double check
2. 可以为zset设置一个过期时间，当有请求访问该zset时，若访问时间相比于zset的过期时间小于阈值，则后延过期时间

综合考虑，实践中，方案2优先考虑

## 优化方案

思路：
由于业务逻辑均有产品经理提出，需求是可控的，可以合理假设N的最大值不超过200，构建容量为200的小顶堆来优化上述逻辑

### 方案1

1. 系统启动时，把全部点赞数据从数据库中读出来，写到redis的zset中，然后向消息队列发送消息
2. 每个服务实例监听消息队列，然后读取zset的前200个记录，到服务实例本地内存容量大小为200的小顶堆中
3. 获取点赞数为前N的文章列表是，直接从小顶堆获取数据即可，若极小概率发生N大于200的情况，则按1-3步构建更大的小顶堆
4. 有新的点赞操作时，先写数据库，然后发送消息到队列，异步更新每个实例上的小顶堆


### 方案2

还是采用方案1 的思路，直接把zset改造为优先队列，即给zset额外设置一个capacity值，如200。
从数据库中读取每篇文章的点赞数，但zset中元素少于capacity时，均进入zset；当zset元素总数等于capacity后，若当前数据小于zset最后一个元素，则将元素抛弃；若大于最后一个元素，则将元素加入zset，并移除最后一个元素。

注：
上述操作，可以免去锁操作来增强性能：
1. 由于对redis的操作，在redis中是单线程执行，所以所有的操作在redis中都会串行执行，有数据入队，就有不同的数据出队，不会在出入队上由于并发问题导致出队错误
2. 先插入数据，再删除队尾多出来的数据，这样数据也是安全的
3. 第2步操作可能出现并发问题的地方在于，看到队尾元素小于当前元素，准备把当前元素插入队列时，另一个线程已经插入过数据，这时队尾数据已经比当前数据大，不应该再插入该数据了，但由于当前线程不知道，还是执行插入操作。
4. 不过第3步描述的情况这里不会出现数据错误问题，由于zset是有序的，在3的情况下，插入数据是插入到队尾，删除数据也是删除队尾数据，相当于这条数据还是被抛弃了
5. 


# 作业详情

作业按基础方案来实现，使用了方案2的优化手段

在repository, cache, dao和service中均补充了getNLike 相关的实现，单个文章的喜欢功能还没修改